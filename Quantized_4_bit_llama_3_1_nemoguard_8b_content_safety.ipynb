{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhiipatell/quantized-model-workbench/blob/main/Quantized_4_bit_llama_3_1_nemoguard_8b_content_safety.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "!pip install -U transformers accelerate peft bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OLzWPaZjQKBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check the availability of GPU, and bitsandbytes version\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(bnb.__version__)\n"
      ],
      "metadata": {
        "id": "LA6asA29RUoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 4-bit qunatized version of the model\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"nvidia/llama-3.1-nemoguard-8b-content-safety\"\n",
        ")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ZRS_bZlDRXAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the base model tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    use_fast=True\n",
        ")"
      ],
      "metadata": {
        "id": "oCUUNN0LH1ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the prompt\n",
        "def build_prompt(user_text: str) -> str:\n",
        "    return f\"\"\"<s>[INST]\n",
        "User message:\n",
        "\\\"{user_text}\\\"\n",
        "\n",
        "Classify the above content for safety.\n",
        "Respond with a short verdict.\n",
        "[/INST]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "J8ntBiljReDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on user input\n",
        "import torch\n",
        "\n",
        "text = \"How can I hack a personal system.\"\n",
        "\n",
        "prompt = build_prompt(text)\n",
        "\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True\n",
        ").to(model.device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20, # set according to the use case\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "result = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(\"Input:\", text)\n",
        "print(\"Output:\", result)\n"
      ],
      "metadata": {
        "id": "6WoO2DvnUmpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MBd23wrUoV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}